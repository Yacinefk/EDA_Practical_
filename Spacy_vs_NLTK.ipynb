{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\belma\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (4.60.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\belma\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\belma\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc = nlp(\"Mr. Yacine has become a Data scientist. he's working from home and he make $65 an hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Yacine\n",
      "has\n",
      "become\n",
      "a\n",
      "Data\n",
      "scientist\n",
      ".\n",
      "he\n",
      "'s\n",
      "working\n",
      "from\n",
      "home\n",
      "and\n",
      "he\n",
      "make\n",
      "$\n",
      "65\n",
      "an\n",
      "hour\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2=doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'has'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. ==> index 0 is_alpha False is_punct False like_num False is_currency False\n",
      "Yacine ==> index 1 is_alpha True is_punct False like_num False is_currency False\n",
      "has ==> index 2 is_alpha True is_punct False like_num False is_currency False\n",
      "become ==> index 3 is_alpha True is_punct False like_num False is_currency False\n",
      "a ==> index 4 is_alpha True is_punct False like_num False is_currency False\n",
      "Data ==> index 5 is_alpha True is_punct False like_num False is_currency False\n",
      "scientist ==> index 6 is_alpha True is_punct False like_num False is_currency False\n",
      ". ==> index 7 is_alpha False is_punct True like_num False is_currency False\n",
      "he ==> index 8 is_alpha True is_punct False like_num False is_currency False\n",
      "'s ==> index 9 is_alpha False is_punct False like_num False is_currency False\n",
      "working ==> index 10 is_alpha True is_punct False like_num False is_currency False\n",
      "from ==> index 11 is_alpha True is_punct False like_num False is_currency False\n",
      "home ==> index 12 is_alpha True is_punct False like_num False is_currency False\n",
      "and ==> index 13 is_alpha True is_punct False like_num False is_currency False\n",
      "he ==> index 14 is_alpha True is_punct False like_num False is_currency False\n",
      "make ==> index 15 is_alpha True is_punct False like_num False is_currency False\n",
      "$ ==> index 16 is_alpha False is_punct False like_num False is_currency True\n",
      "65 ==> index 17 is_alpha False is_punct False like_num True is_currency False\n",
      "an ==> index 18 is_alpha True is_punct False like_num False is_currency False\n",
      "hour ==> index 19 is_alpha True is_punct False like_num False is_currency False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, '==>', 'index', token.i, \n",
    "          'is_alpha', token.is_alpha,\n",
    "         'is_punct', token.is_punct,\n",
    "         'like_num', token.like_num,\n",
    "         'is_currency',token.is_currency\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees informations:\\n',\n",
       " '===========================\\n',\n",
       " '\\n',\n",
       " 'first name    last_name      email\\n',\n",
       " '\\n',\n",
       " 'yacine        belmadani      yacine@belma.com\\n',\n",
       " 'john          week           john@week.com\\n',\n",
       " 'lan           wesler         lan@wesler.com\\n',\n",
       " 'kim           taylor         kim@taylor.com\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('emp_info') as f:\n",
    "    text = f.readlines()\n",
    "text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employees informations:\\n ===========================\\n \\n first name    last_name      email\\n \\n yacine        belmadani      yacine@belma.com\\n john          week           john@week.com\\n lan           wesler         lan@wesler.com\\n kim           taylor         kim@taylor.com\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_emails = []\n",
    "emp_age = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emp_emails.append(token)\n",
    "    elif token.like_num:\n",
    "        emp_age.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yacine@belma.com, john@week.com, lan@wesler.com, kim@taylor.com]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[first]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yacine,\n",
       " is,\n",
       " learning,\n",
       " to,\n",
       " get,\n",
       " data,\n",
       " science,\n",
       " job,\n",
       " .,\n",
       " yacine,\n",
       " wants,\n",
       " a,\n",
       " machine,\n",
       " learning,\n",
       " engineer]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are you doing here in the middle of the night.\n",
      "you can get yourself in danger\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x277fa187440>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yacine is learning to get data science job.\n",
      "yacine wants a machine learning engineer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research id often freely available. Good places\n",
    "to start iclude http://www.data.gov/, and http://www.science.gov/, and the United kingdom, http://data.gov,uk/.\n",
    "two of my favorite data sets are General Social Survey at http://www3.norc.org/gss+website/,\n",
    "and the european Social Survey at http://europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science.gov/',\n",
       " 'http://data.gov',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        urls.append(token.text)\n",
    "urls        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = 'Tony gave two $ to Peter, Bruce gave 500 € to Steve'\n",
    "doc = nlp(transactions)\n",
    "\n",
    "M_currency = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_currency or token.like_num:\n",
    "        \n",
    "        M_currency.append(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[two, $, 500, €]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token, doc[token.i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x277fbc20040>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x277fbc20460>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x277fba72eb0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x277fbbb0e00>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x277fbc8fe00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x277fba72d60>)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yacine   NOUN   yacine\n",
      "is   AUX   be\n",
      "learning   VERB   learn\n",
      "to   PART   to\n",
      "get   VERB   get\n",
      "data   NOUN   datum\n",
      "science   NOUN   science\n",
      "job   NOUN   job\n",
      ".   PUNCT   .\n",
      "yacine   NOUN   yacine\n",
      "wants   VERB   want\n",
      "a   DET   a\n",
      "machine   NOUN   machine\n",
      "learning   VERB   learn\n",
      "engineer   NOUN   engineer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, ' ', token.pos_,' ', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    steve jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " has made \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    300$ billions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in hhis life</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('steve jobs has made 300$ billions in hhis life')\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Data science has been ranked the second best job in united states in 2019. a data scientist can earn $50 an hour as base salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data | NOUN | True | datum\n",
      "science | NOUN | True | science\n",
      "has | AUX | True | have\n",
      "been | AUX | True | be\n",
      "ranked | VERB | True | rank\n",
      "the | DET | True | the\n",
      "second | ADJ | True | second\n",
      "best | ADJ | True | good\n",
      "job | NOUN | True | job\n",
      "in | ADP | True | in\n",
      "united | PROPN | True | united\n",
      "states | PROPN | True | states\n",
      "in | ADP | True | in\n",
      "2019 | NUM | False | 2019\n",
      ". | PUNCT | False | .\n",
      "a | DET | True | a\n",
      "data | NOUN | True | data\n",
      "scientist | NOUN | True | scientist\n",
      "can | AUX | True | can\n",
      "earn | VERB | True | earn\n",
      "$ | SYM | False | $\n",
      "50 | NUM | False | 50\n",
      "an | DET | True | an\n",
      "hour | NOUN | True | hour\n",
      "as | ADP | True | as\n",
      "base | NOUN | True | base\n",
      "salary | NOUN | True | salary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for  token in doc:\n",
    "    print(token, '|',token.pos_,'|' ,token.is_alpha, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming, Lemmatization with nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'eats', 'eat', 'ate', 'adjustable','rafting', 'ability','meeting']\n",
    "for word in words:\n",
    "    print(word,'|',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating eats eat ate adjustable rafting ability meeting'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "for word in words:\n",
    "    doc = ' '.join(words)\n",
    "doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eating\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meeting\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(doc)\n",
    "for token in doc:\n",
    "    print(token,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bro | brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "brah | brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{'text':'bro'}], [{'text':'brah'}]], {'lemma':'brother'})\n",
    "\n",
    "doc = nlp(\"bro, you wanna go? brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | proper noun | NNP | noun, proper singular\n",
      "flew | VERB | verb | VBD | verb, past tense\n",
      "to | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "mars | NOUN | noun | NNS | noun, plural\n",
      "yesterday | NOUN | noun | NN | noun, singular or mass\n",
      ". | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "He | PRON | pronoun | PRP | pronoun, personal\n",
      "carried | VERB | verb | VBD | verb, past tense\n",
      "sushi | PROPN | proper noun | NNP | noun, proper singular\n",
      "with | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "him | PRON | pronoun | PRP | pronoun, personal\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Elon flew to mars yesterday. He carried sushi with him')\n",
    "for token in doc:\n",
    "    print(token, '|', token.pos_, '|', spacy.explain(token.pos_), '|',token.tag_,\n",
    "          '|', spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_text = \"\"\"Microsoft Corp. today announced the following results for the quarter ended June 30, 2022, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.9 billion and increased 12% (up 16% in constant currency)\n",
    "\n",
    "·         Operating income was $20.5 billion and increased 8% (up 14% in constant currency)\n",
    "\n",
    "·         Net income was $16.7 billion and increased 2% (up 7% in constant currency)\n",
    "\n",
    "·         Diluted earnings per share was $2.23 and increased 3% (up 8% in constant currency)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microsoft Corp. today announced the following results for the quarter ended June 30, 2022, as compared to the corresponding period of last fiscal year:\\n\\n·         Revenue was $51.9 billion and increased 12% (up 16% in constant currency)\\n\\n·         Operating income was $20.5 billion and increased 8% (up 14% in constant currency)\\n\\n·         Net income was $16.7 billion and increased 2% (up 7% in constant currency)\\n\\n·         Diluted earnings per share was $2.23 and increased 3% (up 8% in constant currency)\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "doc = nlp(earning_text)\n",
    "\n",
    "filtered_data = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in ['SPACE', 'X', 'PUNCT']:\n",
    "        filtered_data.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter,\n",
       " ended,\n",
       " June,\n",
       " 30,\n",
       " 2022,\n",
       " as,\n",
       " compared,\n",
       " to,\n",
       " the,\n",
       " corresponding,\n",
       " period,\n",
       " of,\n",
       " last,\n",
       " fiscal,\n",
       " year,\n",
       " Revenue,\n",
       " was,\n",
       " $,\n",
       " 51.9,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 12,\n",
       " %,\n",
       " up,\n",
       " 16,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Operating,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 20.5,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 8,\n",
       " %,\n",
       " up,\n",
       " 14,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Net,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 16.7,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 2,\n",
       " %,\n",
       " up,\n",
       " 7,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Diluted,\n",
       " earnings,\n",
       " per,\n",
       " share,\n",
       " was,\n",
       " $,\n",
       " 2.23,\n",
       " and,\n",
       " increased,\n",
       " 3,\n",
       " %,\n",
       " up,\n",
       " 8,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 4,\n",
       " 92: 21,\n",
       " 100: 9,\n",
       " 90: 3,\n",
       " 85: 8,\n",
       " 93: 17,\n",
       " 97: 15,\n",
       " 98: 1,\n",
       " 84: 9,\n",
       " 103: 9,\n",
       " 87: 4,\n",
       " 99: 4,\n",
       " 89: 4,\n",
       " 86: 4}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FLAG60'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[60].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 4\n",
      "NOUN | 21\n",
      "VERB | 9\n",
      "DET | 3\n",
      "ADP | 8\n",
      "NUM | 17\n",
      "PUNCT | 15\n",
      "SCONJ | 1\n",
      "ADJ | 9\n",
      "SPACE | 9\n",
      "AUX | 4\n",
      "SYM | 4\n",
      "CCONJ | 4\n",
      "ADV | 4\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, '|', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from Text File\n",
    "with open ('news_story','r') as f:\n",
    "    news_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\\\n\\\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from MarchÃ¢â‚¬â„¢s peak but was still close to the highest level since the summer of 1982.\\\\n\\\\nRemoving volatile food and ene'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Inflation,\n",
       " climb,\n",
       " consumers,\n",
       " brink,\n",
       " expansion,\n",
       " consumer,\n",
       " price,\n",
       " index,\n",
       " measure,\n",
       " prices,\n",
       " goods,\n",
       " services,\n",
       " %,\n",
       " year,\n",
       " estimate,\n",
       " %,\n",
       " gain,\n",
       " ease,\n",
       " peak,\n",
       " level,\n",
       " summer,\n",
       " food,\n",
       " ene]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all NOUN tokens from the story in python list\n",
    "Noun_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ in 'NOUN':\n",
    "        Noun_list.append(token)\n",
    "Noun_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.3, 8.1]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all numbers in python list\n",
    "Number_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ in 'NUM':\n",
    "        Number_list.append(token)\n",
    "Number_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 23,\n",
       " 100: 9,\n",
       " 86: 4,\n",
       " 85: 11,\n",
       " 96: 8,\n",
       " 97: 7,\n",
       " 90: 11,\n",
       " 95: 2,\n",
       " 87: 3,\n",
       " 89: 4,\n",
       " 84: 6,\n",
       " 93: 2,\n",
       " 98: 1}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the POS tags in the story\n",
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN ====== 23\n",
      "VERB ====== 9\n",
      "ADV ====== 4\n",
      "ADP ====== 11\n",
      "PROPN ====== 8\n",
      "PUNCT ====== 7\n",
      "DET ====== 11\n",
      "PRON ====== 2\n",
      "AUX ====== 3\n",
      "CCONJ ====== 4\n",
      "ADJ ====== 6\n",
      "NUM ====== 2\n",
      "SCONJ ====== 1\n"
     ]
    }
   ],
   "source": [
    "doc.count_by(spacy.attrs.POS)\n",
    "for k, v in count.items():\n",
    "    print(doc.vocab[k].text, '======', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp ('what are you doing here in the middle of the night. you can get yourself in danger')\n",
    "stop_words = [token for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[middle, night, danger]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    not_stop = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return not_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"data science is multifield that combines programming, mathematics and domain knowledge to extract miningful insights from data \"\n",
    "sent = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'multifield',\n",
       " 'combines',\n",
       " 'programming',\n",
       " 'mathematics',\n",
       " 'domain',\n",
       " 'knowledge',\n",
       " 'extract',\n",
       " 'miningful',\n",
       " 'insights',\n",
       " 'data']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    stop = [token.text for token in doc if token.is_stop]\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'that', 'and', 'to', 'from']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_preprocess(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
