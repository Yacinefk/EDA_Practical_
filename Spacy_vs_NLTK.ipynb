{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc = nlp(\"Mr. Yacine has become a Data scientist. he's working from home and he make $65 an hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Yacine\n",
      "has\n",
      "become\n",
      "a\n",
      "Data\n",
      "scientist\n",
      ".\n",
      "he\n",
      "'s\n",
      "working\n",
      "from\n",
      "home\n",
      "and\n",
      "he\n",
      "make\n",
      "$\n",
      "65\n",
      "an\n",
      "hour\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use dir() to see what are all the options we have\n",
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. ==> index 0 is_alpha False is_punct False like_num False is_currency False\n",
      "Yacine ==> index 1 is_alpha True is_punct False like_num False is_currency False\n",
      "has ==> index 2 is_alpha True is_punct False like_num False is_currency False\n",
      "become ==> index 3 is_alpha True is_punct False like_num False is_currency False\n",
      "a ==> index 4 is_alpha True is_punct False like_num False is_currency False\n",
      "Data ==> index 5 is_alpha True is_punct False like_num False is_currency False\n",
      "scientist ==> index 6 is_alpha True is_punct False like_num False is_currency False\n",
      ". ==> index 7 is_alpha False is_punct True like_num False is_currency False\n",
      "he ==> index 8 is_alpha True is_punct False like_num False is_currency False\n",
      "'s ==> index 9 is_alpha False is_punct False like_num False is_currency False\n",
      "working ==> index 10 is_alpha True is_punct False like_num False is_currency False\n",
      "from ==> index 11 is_alpha True is_punct False like_num False is_currency False\n",
      "home ==> index 12 is_alpha True is_punct False like_num False is_currency False\n",
      "and ==> index 13 is_alpha True is_punct False like_num False is_currency False\n",
      "he ==> index 14 is_alpha True is_punct False like_num False is_currency False\n",
      "make ==> index 15 is_alpha True is_punct False like_num False is_currency False\n",
      "$ ==> index 16 is_alpha False is_punct False like_num False is_currency True\n",
      "65 ==> index 17 is_alpha False is_punct False like_num True is_currency False\n",
      "an ==> index 18 is_alpha True is_punct False like_num False is_currency False\n",
      "hour ==> index 19 is_alpha True is_punct False like_num False is_currency False\n"
     ]
    }
   ],
   "source": [
    "# Run a for loop to explore some of the options that we can perform on the text\n",
    "for token in doc:\n",
    "    print(token, '==>', 'index', token.i, \n",
    "          'is_alpha', token.is_alpha,\n",
    "         'is_punct', token.is_punct,\n",
    "         'like_num', token.like_num,\n",
    "         'is_currency',token.is_currency\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employees informations:\\n',\n",
       " '===========================\\n',\n",
       " '\\n',\n",
       " 'first name    last_name      email                   age\\n',\n",
       " 'yacine        belmadani      yacine@belma.com        33\\n',\n",
       " 'john          week           john@week.com           33\\n',\n",
       " 'lan           wesler         lan@wesler.com          45\\n',\n",
       " 'kim           taylor         kim@taylor.com          40\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use with open to read employees info \n",
    "with open('emp_info') as f:\n",
    "    text = f.readlines()\n",
    "text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employees informations:\\n ===========================\\n \\n first name    last_name      email                   age\\n yacine        belmadani      yacine@belma.com        33\\n john          week           john@week.com           33\\n lan           wesler         lan@wesler.com          45\\n kim           taylor         kim@taylor.com          40\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the doc \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the emails of the employees\n",
    "emp_emails = []\n",
    "emp_age = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emp_emails.append(token)\n",
    "    elif token.like_num:\n",
    "        emp_age.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yacine@belma.com, john@week.com, lan@wesler.com, kim@taylor.com]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[first, 33, 33, 45, 40]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yacine,\n",
       " is,\n",
       " learning,\n",
       " to,\n",
       " get,\n",
       " data,\n",
       " science,\n",
       " job,\n",
       " .,\n",
       " yacine,\n",
       " wants,\n",
       " a,\n",
       " machine,\n",
       " learning,\n",
       " engineer]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1fd8034f280>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yacine is learning to get data science job.\n",
      "yacine wants a machine learning engineer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research id often freely available. Good places\n",
    "to start iclude http://www.data.gov/, and http://www.science.gov/, and the United kingdom, http://data.gov,uk/.\n",
    "two of my favorite data sets are General Social Survey at http://www3.norc.org/gss+website/,\n",
    "and the european Social Survey at http://europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science.gov/',\n",
       " 'http://data.gov',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        urls.append(token.text)\n",
    "urls        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the currency and the numbers\n",
    "transactions = 'Tony gave two $ to Peter, Bruce gave 500 € to Steve'\n",
    "doc = nlp(transactions)\n",
    "\n",
    "M_currency = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_currency or token.like_num:\n",
    "        \n",
    "        M_currency.append(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[two, $, 500, €]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token, doc[token.i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1fd80082700>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1fd8022de80>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1fd80460040>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1fd80759100>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1fd807640c0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1fd80460970>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yacine   NOUN   yacine\n",
      "is   AUX   be\n",
      "learning   VERB   learn\n",
      "to   PART   to\n",
      "get   VERB   get\n",
      "data   NOUN   datum\n",
      "science   NOUN   science\n",
      "job   NOUN   job\n",
      ".   PUNCT   .\n",
      "yacine   NOUN   yacine\n",
      "wants   VERB   want\n",
      "a   DET   a\n",
      "machine   NOUN   machine\n",
      "learning   VERB   learn\n",
      "engineer   NOUN   engineer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('yacine is learning to get data science job. yacine wants a machine learning engineer')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, ' ', token.pos_,' ', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    steve jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " has made \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    300$ billions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in hhis life</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('steve jobs has made 300$ billions in hhis life')\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Data science has been ranked the second best job in united states in 2019. a data scientist can earn $50 an hour as base salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data | NOUN | True | datum\n",
      "science | NOUN | True | science\n",
      "has | AUX | True | have\n",
      "been | AUX | True | be\n",
      "ranked | VERB | True | rank\n",
      "the | DET | True | the\n",
      "second | ADJ | True | second\n",
      "best | ADJ | True | good\n",
      "job | NOUN | True | job\n",
      "in | ADP | True | in\n",
      "united | PROPN | True | united\n",
      "states | PROPN | True | states\n",
      "in | ADP | True | in\n",
      "2019 | NUM | False | 2019\n",
      ". | PUNCT | False | .\n",
      "a | DET | True | a\n",
      "data | NOUN | True | data\n",
      "scientist | NOUN | True | scientist\n",
      "can | AUX | True | can\n",
      "earn | VERB | True | earn\n",
      "$ | SYM | False | $\n",
      "50 | NUM | False | 50\n",
      "an | DET | True | an\n",
      "hour | NOUN | True | hour\n",
      "as | ADP | True | as\n",
      "base | NOUN | True | base\n",
      "salary | NOUN | True | salary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for  token in doc:\n",
    "    print(token, '|',token.pos_,'|' ,token.is_alpha, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming, Lemmatization with nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'eats', 'eat', 'ate', 'adjustable','rafting', 'ability','meeting']\n",
    "for word in words:\n",
    "    print(word,'|',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating eats eat ate adjustable rafting ability meeting'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "for word in words:\n",
    "    doc = ' '.join(words)\n",
    "doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eating\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meeting\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(doc)\n",
    "for token in doc:\n",
    "    print(token,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bro | brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "brah | brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{'text':'bro'}], [{'text':'brah'}]], {'lemma':'brother'})\n",
    "\n",
    "doc = nlp(\"bro, you wanna go? brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | proper noun | NNP | noun, proper singular\n",
      "flew | VERB | verb | VBD | verb, past tense\n",
      "to | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "mars | NOUN | noun | NNS | noun, plural\n",
      "yesterday | NOUN | noun | NN | noun, singular or mass\n",
      ". | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "He | PRON | pronoun | PRP | pronoun, personal\n",
      "carried | VERB | verb | VBD | verb, past tense\n",
      "sushi | PROPN | proper noun | NNP | noun, proper singular\n",
      "with | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "him | PRON | pronoun | PRP | pronoun, personal\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Elon flew to mars yesterday. He carried sushi with him')\n",
    "for token in doc:\n",
    "    print(token, '|', token.pos_, '|', spacy.explain(token.pos_), '|',token.tag_,\n",
    "          '|', spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_text = \"\"\"Microsoft Corp. today announced the following results for the quarter ended June 30, 2022, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.9 billion and increased 12% (up 16% in constant currency)\n",
    "\n",
    "·         Operating income was $20.5 billion and increased 8% (up 14% in constant currency)\n",
    "\n",
    "·         Net income was $16.7 billion and increased 2% (up 7% in constant currency)\n",
    "\n",
    "·         Diluted earnings per share was $2.23 and increased 3% (up 8% in constant currency)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microsoft Corp. today announced the following results for the quarter ended June 30, 2022, as compared to the corresponding period of last fiscal year:\\n\\n·         Revenue was $51.9 billion and increased 12% (up 16% in constant currency)\\n\\n·         Operating income was $20.5 billion and increased 8% (up 14% in constant currency)\\n\\n·         Net income was $16.7 billion and increased 2% (up 7% in constant currency)\\n\\n·         Diluted earnings per share was $2.23 and increased 3% (up 8% in constant currency)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "doc = nlp(earning_text)\n",
    "\n",
    "filtered_data = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in ['SPACE', 'X', 'PUNCT']:\n",
    "        filtered_data.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter,\n",
       " ended,\n",
       " June,\n",
       " 30,\n",
       " 2022,\n",
       " as,\n",
       " compared,\n",
       " to,\n",
       " the,\n",
       " corresponding,\n",
       " period,\n",
       " of,\n",
       " last,\n",
       " fiscal,\n",
       " year,\n",
       " Revenue,\n",
       " was,\n",
       " $,\n",
       " 51.9,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 12,\n",
       " %,\n",
       " up,\n",
       " 16,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Operating,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 20.5,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 8,\n",
       " %,\n",
       " up,\n",
       " 14,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Net,\n",
       " income,\n",
       " was,\n",
       " $,\n",
       " 16.7,\n",
       " billion,\n",
       " and,\n",
       " increased,\n",
       " 2,\n",
       " %,\n",
       " up,\n",
       " 7,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency,\n",
       " Diluted,\n",
       " earnings,\n",
       " per,\n",
       " share,\n",
       " was,\n",
       " $,\n",
       " 2.23,\n",
       " and,\n",
       " increased,\n",
       " 3,\n",
       " %,\n",
       " up,\n",
       " 8,\n",
       " %,\n",
       " in,\n",
       " constant,\n",
       " currency]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 4,\n",
       " 92: 21,\n",
       " 100: 9,\n",
       " 90: 3,\n",
       " 85: 8,\n",
       " 93: 17,\n",
       " 97: 15,\n",
       " 98: 1,\n",
       " 84: 9,\n",
       " 103: 9,\n",
       " 87: 4,\n",
       " 99: 4,\n",
       " 89: 4,\n",
       " 86: 4}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FLAG60'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[60].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 4\n",
      "NOUN | 21\n",
      "VERB | 9\n",
      "DET | 3\n",
      "ADP | 8\n",
      "NUM | 17\n",
      "PUNCT | 15\n",
      "SCONJ | 1\n",
      "ADJ | 9\n",
      "SPACE | 9\n",
      "AUX | 4\n",
      "SYM | 4\n",
      "CCONJ | 4\n",
      "ADV | 4\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, '|', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from Text File\n",
    "with open ('news_story','r') as f:\n",
    "    news_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\\\n\\\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from MarchÃ¢â‚¬â„¢s peak but was still close to the highest level since the summer of 1982.\\\\n\\\\nRemoving volatile food and ene'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Inflation,\n",
       " climb,\n",
       " consumers,\n",
       " brink,\n",
       " expansion,\n",
       " consumer,\n",
       " price,\n",
       " index,\n",
       " measure,\n",
       " prices,\n",
       " goods,\n",
       " services,\n",
       " %,\n",
       " year,\n",
       " estimate,\n",
       " %,\n",
       " gain,\n",
       " ease,\n",
       " peak,\n",
       " level,\n",
       " summer,\n",
       " food,\n",
       " ene]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all NOUN tokens from the story in python list\n",
    "Noun_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ in 'NOUN':\n",
    "        Noun_list.append(token)\n",
    "Noun_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.3, 8.1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all numbers in python list\n",
    "Number_list = []\n",
    "for token in doc:\n",
    "    if token.pos_ in 'NUM':\n",
    "        Number_list.append(token)\n",
    "Number_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 23,\n",
       " 100: 9,\n",
       " 86: 4,\n",
       " 85: 11,\n",
       " 96: 8,\n",
       " 97: 7,\n",
       " 90: 11,\n",
       " 95: 2,\n",
       " 87: 3,\n",
       " 89: 4,\n",
       " 84: 6,\n",
       " 93: 2,\n",
       " 98: 1}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the POS tags in the story\n",
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN ====== 23\n",
      "VERB ====== 9\n",
      "ADV ====== 4\n",
      "ADP ====== 11\n",
      "PROPN ====== 8\n",
      "PUNCT ====== 7\n",
      "DET ====== 11\n",
      "PRON ====== 2\n",
      "AUX ====== 3\n",
      "CCONJ ====== 4\n",
      "ADJ ====== 6\n",
      "NUM ====== 2\n",
      "SCONJ ====== 1\n"
     ]
    }
   ],
   "source": [
    "doc.count_by(spacy.attrs.POS)\n",
    "for k, v in count.items():\n",
    "    print(doc.vocab[k].text, '======', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp ('what are you doing here in the middle of the night. you can get yourself in danger')\n",
    "stop_words = [token for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[middle, night, danger]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function with list comprehension that remove the stopwords and punctuations from the text\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    not_stop = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return not_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"data science is multifield that combines programming, mathematics and domain knowledge to extract miningful insights from data \"\n",
    "sent = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'multifield',\n",
       " 'combines',\n",
       " 'programming',\n",
       " 'mathematics',\n",
       " 'domain',\n",
       " 'knowledge',\n",
       " 'extract',\n",
       " 'miningful',\n",
       " 'insights',\n",
       " 'data']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    stop = [token.text for token in doc if token.is_stop]\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'that', 'and', 'to', 'from']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_preprocess(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
